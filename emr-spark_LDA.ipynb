{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial imports\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark as ps    # for the pyspark suite\n",
    "\n",
    "\n",
    "#NLP imports\n",
    "from src.nltk_pipe import indexing_pipeline\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up spark\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "            .master(\"local[4]\") \\\n",
    "            .appName(\"df JulesVerne\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "import sys\n",
    "\n",
    "def extract_bow_from_raw_text(text_as_string):\n",
    "    \"\"\"Extracts bag-of-words from a raw text string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text (str): a text document given as a string\n",
    "    Returns\n",
    "    -------\n",
    "    list : the list of the tokens extracted and filtered from the text\n",
    "    \"\"\"\n",
    "    if (text_as_string == None):\n",
    "        return []\n",
    "\n",
    "    if (len(text_as_string) < 1):\n",
    "        return []\n",
    "\n",
    "    if sys.version_info[0] < 3:\n",
    "        nfkd_form = unicodedata.normalize('NFKD', unicode(text_as_string))\n",
    "    else:\n",
    "        nfkd_form = unicodedata.normalize('NFKD', str(text_as_string))\n",
    "\n",
    "    text_input = str(nfkd_form.encode('ASCII', 'ignore'))\n",
    "\n",
    "    sent_tokens = sent_tokenize(text_input)\n",
    "\n",
    "    tokens = list(map(word_tokenize, sent_tokens))\n",
    "\n",
    "    sent_tags = list(map(pos_tag, tokens))\n",
    "\n",
    "    grammar = r\"\"\"\n",
    "        SENT: {<(J|N).*>}                # chunk sequences of proper nouns\n",
    "    \"\"\"\n",
    "\n",
    "    cp = RegexpParser(grammar)\n",
    "    ret_tokens = list()\n",
    "    stemmer_snowball = SnowballStemmer('english')\n",
    "\n",
    "    for sent in sent_tags:\n",
    "        tree = cp.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'SENT':\n",
    "                t_tokenlist = [tpos[0].lower() for tpos in subtree.leaves()]\n",
    "                t_tokens_stemsnowball = list(map(stemmer_snowball.stem, t_tokenlist))\n",
    "                #t_token = \"-\".join(t_tokens_stemsnowball)\n",
    "                #ret_tokens.append(t_token)\n",
    "                ret_tokens.extend(t_tokens_stemsnowball)\n",
    "            #if subtree.label() == 'V2V': print(subtree)\n",
    "    #tokens_lower = [map(string.lower, sent) for sent in tokens]\n",
    "\n",
    "    return(ret_tokens)\n",
    "\n",
    "\n",
    "def indexing_pipeline(input_df, **kwargs):\n",
    "    \"\"\"Runs a full text indexing pipeline on a collection of texts contained in a DataFrame.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df (DataFrame): a DataFrame that contains a field called 'text'\n",
    "    Returns\n",
    "    -------\n",
    "    df : the same DataFrames with a column called 'features' for each document\n",
    "    wordlist : the list of words in the vocabulary with their corresponding IDF\n",
    "    \"\"\"\n",
    "    inputCol_ = kwargs.get(\"inputCol\", \"reviews\")\n",
    "    vocabSize_ = kwargs.get(\"vocabSize\", 5000)\n",
    "    minDF_ = kwargs.get(\"minDF\", 2.0)\n",
    "\n",
    "    # ugly: to add that to our slave nodes so that it finds the bootstrapped nltk_data\n",
    "    nltk.data.path.append('/home/hadoop/nltk_data')\n",
    "\n",
    "    extract_bow_from_raw_text(\"\")  # ugly: for instanciating all dependencies of this function\n",
    "    tokenizer_udf = udf(extract_bow_from_raw_text, ArrayType(StringType()))\n",
    "    df_tokens = input_df.withColumn(\"bow\", tokenizer_udf(col(inputCol_)))\n",
    "\n",
    "    cv = CountVectorizer(inputCol=\"bow\", outputCol=\"vector_tf\", vocabSize=vocabSize_, minDF=minDF_)\n",
    "    cv_model = cv.fit(df_tokens)\n",
    "    df_features_tf = cv_model.transform(df_tokens)\n",
    "\n",
    "    idf = IDF(inputCol=\"vector_tf\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(df_features_tf)\n",
    "    df_features = idfModel.transform(df_features_tf)\n",
    "\n",
    "    return(df_features, cv_model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reviews_unique.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = df.reviews\n",
    "ls_attr = df.attractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_total, ls_total = indexing_pipeline(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_total.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'detail',\n",
       " u'center',\n",
       " u'summer',\n",
       " u'right',\n",
       " u'servic',\n",
       " u'light',\n",
       " u'happi',\n",
       " u'lunch',\n",
       " u'highlight',\n",
       " u'war',\n",
       " u'step',\n",
       " u'station',\n",
       " u'ancient',\n",
       " u'centr',\n",
       " u'varieti',\n",
       " u'stun',\n",
       " u'onli',\n",
       " u'money',\n",
       " u'magnific',\n",
       " u'even',\n",
       " u'least',\n",
       " u'entri',\n",
       " u'opera',\n",
       " u'trail',\n",
       " u'set',\n",
       " u'drink',\n",
       " u'paint',\n",
       " u'sun',\n",
       " u'noth',\n",
       " u'adult',\n",
       " u'insid',\n",
       " u'same',\n",
       " u'second',\n",
       " u'opportun',\n",
       " u'piec',\n",
       " u'help',\n",
       " u'clear',\n",
       " u'look',\n",
       " u'quiet',\n",
       " u'avail',\n",
       " u'reason',\n",
       " u'perform',\n",
       " u'space',\n",
       " u'path',\n",
       " u'bar',\n",
       " u'number',\n",
       " u'sculptur',\n",
       " u'hard',\n",
       " u'feel',\n",
       " u'sceneri',\n",
       " u'plant',\n",
       " u'eleph',\n",
       " u'gorgeous',\n",
       " u'week',\n",
       " u'modern',\n",
       " u'floor',\n",
       " u'flower',\n",
       " u'famous',\n",
       " u'middl',\n",
       " u'waterfal',\n",
       " u'make',\n",
       " u'cruis',\n",
       " u'queue',\n",
       " u'dolphin',\n",
       " u'travel',\n",
       " u'ship',\n",
       " u'import',\n",
       " u'young',\n",
       " u'various',\n",
       " u'relax',\n",
       " u'home',\n",
       " u'villag',\n",
       " u'audio',\n",
       " u'fall',\n",
       " u'close',\n",
       " u'countri',\n",
       " u'structur',\n",
       " u'cave',\n",
       " u'theatr',\n",
       " u'pleas',\n",
       " u'aquarium',\n",
       " u'cours',\n",
       " u'wife',\n",
       " u'disappoint',\n",
       " u'must',\n",
       " u'seat',\n",
       " u'front',\n",
       " u'fountain',\n",
       " u'live',\n",
       " u'fabul',\n",
       " u'favorit',\n",
       " u'season',\n",
       " u'stair',\n",
       " u'blue',\n",
       " u'coffe',\n",
       " u'mosqu',\n",
       " u'went',\n",
       " u'atmospher',\n",
       " u'lucki',\n",
       " u'entir']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls_total[200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_total = LDA(k=20, optimizer=\"em\")\n",
    "model_total = lda_total.fit(df_total)\n",
    "topics_total = model_total.describeTopics().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- TOPIC 0 -\n",
      "  - word 'build': 0.0623157180202\n",
      "  - word 'new': 0.0424890351302\n",
      "  - word 'island': 0.024226885583\n",
      "  - word 'audio': 0.0184868332883\n",
      "  - word 'children': 0.0178801621863\n",
      "  - word 'hotel': 0.0175890600618\n",
      "  - word 'dear': 0.0160662963942\n",
      "  - word 'boat': 0.0136781364991\n",
      "  - word 'close': 0.0132073301112\n",
      "  - word 'mani': 0.0107445206337\n",
      "- TOPIC 1 -\n",
      "  - word 'templ': 0.0781235369527\n",
      "  - word 'zoo': 0.0757329021635\n",
      "  - word 'sever': 0.0370382179936\n",
      "  - word 'collect': 0.0283182115014\n",
      "  - word 'fantast': 0.019732223729\n",
      "  - word 'road': 0.0163277550923\n",
      "  - word 'much': 0.0156146280723\n",
      "  - word 'climb': 0.0150178690864\n",
      "  - word 'heart': 0.0145702458646\n",
      "  - word 'chanc': 0.0107870421279\n",
      "- TOPIC 2 -\n",
      "  - word 'inform': 0.0330075223316\n",
      "  - word 'big': 0.0181275971522\n",
      "  - word 'photo': 0.0153232248022\n",
      "  - word 'facil': 0.0104507422189\n",
      "  - word 'full': 0.00982983631205\n",
      "  - word 'trip': 0.00903992715895\n",
      "  - word 'memori': 0.00899826595316\n",
      "  - word 'stroll': 0.0065412915939\n",
      "  - word 'hous': 0.00653905245658\n",
      "  - word 'entranc': 0.00623945708562\n",
      "- TOPIC 3 -\n",
      "  - word 'review': 0.0260910498023\n",
      "  - word 'small': 0.0209523930816\n",
      "  - word 'stori': 0.0183780509117\n",
      "  - word 'mani': 0.0129446774494\n",
      "  - word 'uniqu': 0.0105047395051\n",
      "  - word 'hous': 0.00882645175264\n",
      "  - word 'fish': 0.00856656195318\n",
      "  - word 'real': 0.00830157183611\n",
      "  - word 'avail': 0.0080929010409\n",
      "  - word 'import': 0.00708437706606\n",
      "- TOPIC 4 -\n",
      "  - word 'way': 0.0772312035375\n",
      "  - word 'plenti': 0.0373453855387\n",
      "  - word 'tour': 0.0343464324889\n",
      "  - word 'drive': 0.0231018924622\n",
      "  - word 'bridg': 0.0221654723675\n",
      "  - word 'car': 0.0112615295632\n",
      "  - word 'white': 0.0106336847641\n",
      "  - word 'possibl': 0.0105298702413\n",
      "  - word 'architectur': 0.00922676561991\n",
      "  - word 'list': 0.00868658816214\n",
      "- TOPIC 5 -\n",
      "  - word 'excel': 0.0670862827036\n",
      "  - word 'wonder': 0.0462771710089\n",
      "  - word 'great': 0.0393373972992\n",
      "  - word 'car': 0.0303399882589\n",
      "  - word 'stun': 0.0196458018154\n",
      "  - word 'museum': 0.0170046708418\n",
      "  - word 'sound': 0.0137466564152\n",
      "  - word 'experi': 0.0132174581931\n",
      "  - word 'tourist': 0.013075257356\n",
      "  - word 'magic': 0.0120318439445\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(\"- TOPIC {} -\".format(topic[0]))\n",
    "    topickeys = topic[1]\n",
    "    topicvalues = topic[2]\n",
    "    for i in range(len(topickeys)):\n",
    "        print(\"  - word '{}': {}\".format(ls_total[topickeys[i]],topicvalues[i]))\n",
    "#print(\"--- [time elapsed: {}]\".format(time.time()-t_begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_test = reviews[0:len(reviews):50]\n",
    "a_test = ls_attr[0:len(ls_attr):50]\n",
    "df_test = pd.DataFrame({'attractions': a_test, 'reviews': r_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_df_test = spark.createDataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_nlp, ls_test_nlp = indexing_pipeline(spark_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LDA(k=6, optimizer=\"em\")\n",
    "model = lda.fit(df_test_nlp)\n",
    "topics = model.describeTopics().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    print(\"- TOPIC {} -\".format(topic[0]))\n",
    "    topickeys = topic[1]\n",
    "    topicvalues = topic[2]\n",
    "    for i in range(len(topickeys)):\n",
    "        print(\"  - word '{}': {}\".format(ls_test_nlp[topickeys[i]],topicvalues[i]))\n",
    "#print(\"--- [time elapsed: {}]\".format(time.time()-t_begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
